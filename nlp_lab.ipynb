{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfOO0dVaWCYuU4GX8aZG/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vkrishna03/amazon_clone/blob/main/nlp_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Python Program to handle text data using some of the built in functions"
      ],
      "metadata": {
        "id": "Q34LOyKB38n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5fq3eDs3pcY",
        "outputId": "35c7dbda-ac5e-40cf-b09d-eb2d49c3b18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['To', 'be', 'or', 'not', 'to', 'be']\n",
            "['Natural', 'Language', 'Processing', '(NLP)', 'is,', 'defined', 'as', 'subfield', 'of', 'Artificial', 'Intelligence', '(AI)']\n",
            "Natural\n",
            "natural\n",
            "NATURAL\n",
            "Natural\n",
            "\n",
            "\n",
            "Natural 7\n",
            "Language 8\n",
            "Processing 10\n",
            "(NLP) 5\n",
            "is, 3\n",
            "defined 7\n",
            "as 2\n",
            "subfield 8\n",
            "of 2\n",
            "Artificial 10\n",
            "Intelligence 12\n",
            "(AI) 4\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural Language Processing (NLP) is, defined as subfield of Artificial Intelligence (AI)\"\n",
        "text2 = \"To be or not to be\"\n",
        "text3 = \"ouagoda\"\n",
        "\n",
        "len(text) #Length of a text\n",
        "tokens = text2.split() #Splitting a text into tokens\n",
        "print(tokens)\n",
        "\n",
        "text4 = text.split()\n",
        "print(text4)\n",
        "[w for w in text4 if len(w) > 5]\n",
        "[w for w in text4 if w.startswith(\"N\")]\n",
        "[w for w in text4 if w.endswith(\"e\")]\n",
        "\n",
        "text5 = text4[0].title()\n",
        "print(text5)\n",
        "\n",
        "text6 = text4[0].lower()\n",
        "print(text6)\n",
        "\n",
        "text7 = text4[0].upper()\n",
        "print(text7)\n",
        "\n",
        "text8 = \"\".join(text4[0])\n",
        "print(text8)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "for token in text.split():\n",
        "  print(token, end = \" \")\n",
        "  print(len(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization, Stemming\n"
      ],
      "metadata": {
        "id": "PAiXDwPH6yG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('udhr')\n",
        "\n",
        "inputText = u\"List, listed, lists, listing, listings\"\n",
        "words = inputText.lower().split(\",\")  #Tokenisation\n",
        "print(words)\n",
        "\n",
        "porter = nltk.PorterStemmer()   #Steming\n",
        "[porter.stem(t) for t in words]\n",
        "\n",
        "udhr = nltk.corpus.udhr.words(\"English-Latin1\")\n",
        "print(udhr[:20])\n",
        "\n",
        "print([porter.stem(t) for t in udhr[:10]])\n",
        "WNlemma = nltk.WordNetLemmatizer()\n",
        "print([WNlemma.lemmatize(t) for t in udhr[:10]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd4d1_g-5X0B",
        "outputId": "4a1834b5-960d-4819-bb74-385cdf5e008b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Package udhr is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['list', ' listed', ' lists', ' listing', ' listings']\n",
            "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the', 'inherent', 'dignity', 'and', 'of', 'the', 'equal', 'and', 'inalienable', 'rights', 'of']\n",
            "['univers', 'declar', 'of', 'human', 'right', 'preambl', 'wherea', 'recognit', 'of', 'the']\n",
            "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pos Tagging"
      ],
      "metadata": {
        "id": "8Z3HHOD3-VV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "input1 = \"Children shouldn't drink a sugary drink before bed.\"\n",
        "tokens = word_tokenize(input1)\n",
        "tagged = pos_tag(tokens)\n",
        "print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW4RUyor-M8f",
        "outputId": "dd055a44-a6e6-43b9-d674-47994b49768c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Children', 'NNP'), ('should', 'MD'), (\"n't\", 'RB'), ('drink', 'VB'), ('a', 'DT'), ('sugary', 'JJ'), ('drink', 'NN'), ('before', 'IN'), ('bed', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGBa4dcR8jRv",
        "outputId": "a2425716-79cd-4d31-ea52-c3e1b52de5d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Defined Function for n-gram model"
      ],
      "metadata": {
        "id": "rUcPB5CVAg8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_model(sentence,n):\n",
        "  words = sentence.split()\n",
        "  output = []\n",
        "  for i in range(len(words)-n+1):\n",
        "    output.append(words[i:i+n])\n",
        "  return output\n",
        "\n",
        "text = input(\"Enter the sentence: \")\n",
        "result = ngram_model(text,1)\n",
        "print(result)\n",
        "\n",
        "result = ngram_model(text,2)\n",
        "print(result)\n",
        "\n",
        "result = ngram_model(text,3)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udj3QxxeAPCM",
        "outputId": "cbbdfb34-909c-4768-cc6f-39dbf839dd41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence: I am happy because I am learning\n",
            "[['I'], ['am'], ['happy'], ['because'], ['I'], ['am'], ['learning']]\n",
            "[['I', 'am'], ['am', 'happy'], ['happy', 'because'], ['because', 'I'], ['I', 'am'], ['am', 'learning']]\n",
            "[['I', 'am', 'happy'], ['am', 'happy', 'because'], ['happy', 'because', 'I'], ['because', 'I', 'am'], ['I', 'am', 'learning']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Function for n-gram model\n"
      ],
      "metadata": {
        "id": "HjSzUM5RD1f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "text = input(\"Enter the sentence: \")\n",
        "\n",
        "unigrams = ngrams(text.split(),1)\n",
        "print(\"\\nUnigrams: \")\n",
        "for grams in unigrams:\n",
        "  print(grams)\n",
        "\n",
        "bigrams = ngrams(text.split(),2)\n",
        "print(\"\\nBigrams: \")\n",
        "for grams in bigrams:\n",
        "  print(grams)\n",
        "\n",
        "trigrams = ngrams(text.split(),3)\n",
        "print(\"\\nTrigrams: \")\n",
        "for grams in trigrams:\n",
        "  print(grams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p68VSVUMBSaJ",
        "outputId": "ea66eefa-fc1d-4b21-9430-69f2d6ba12a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence: I am happy because I am learning\n",
            "\n",
            "Unigrams: \n",
            "('I',)\n",
            "('am',)\n",
            "('happy',)\n",
            "('because',)\n",
            "('I',)\n",
            "('am',)\n",
            "('learning',)\n",
            "\n",
            "Bigrams: \n",
            "('I', 'am')\n",
            "('am', 'happy')\n",
            "('happy', 'because')\n",
            "('because', 'I')\n",
            "('I', 'am')\n",
            "('am', 'learning')\n",
            "\n",
            "Trigrams: \n",
            "('I', 'am', 'happy')\n",
            "('am', 'happy', 'because')\n",
            "('happy', 'because', 'I')\n",
            "('because', 'I', 'am')\n",
            "('I', 'am', 'learning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N Gram Frequency"
      ],
      "metadata": {
        "id": "nGxTjYwoE-6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def calc_ngram_freq(n_gram):\n",
        "  pair_freq = defaultdict(int)\n",
        "  for pair in n_gram:\n",
        "    pair_freq[pair] += 1\n",
        "  return pair_freq\n",
        "\n",
        "text = input(\"Enter the sentence: \")\n",
        "\n",
        "unigrams = ngrams(text.split(),1)\n",
        "print(\"\\nUnigrams: \")\n",
        "pair_freq = calc_ngram_freq(unigrams)\n",
        "for pair,freq in pair_freq.items():\n",
        "  print(pair, \"->\", str(freq))\n",
        "\n",
        "bigrams = ngrams(text.split(),2)\n",
        "print(\"\\nBigrams: \")\n",
        "pair_freq = calc_ngram_freq(bigrams)\n",
        "for pair,freq in pair_freq.items():\n",
        "  print(pair, \"->\", str(freq))\n",
        "\n",
        "trigrams = ngrams(text.split(),3)\n",
        "print(\"\\nTrigrams: \")\n",
        "pair_freq = calc_ngram_freq(trigrams)\n",
        "for pair,freq in pair_freq.items():\n",
        "  print(pair, \"->\", str(freq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gn_vopEEH0O",
        "outputId": "aeda78f7-cd79-403d-eb9c-46c05609947c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the sentence: I am happy because I am learning\n",
            "\n",
            "Unigrams: \n",
            "('I',) -> 2\n",
            "('am',) -> 2\n",
            "('happy',) -> 1\n",
            "('because',) -> 1\n",
            "('learning',) -> 1\n",
            "\n",
            "Bigrams: \n",
            "('I', 'am') -> 2\n",
            "('am', 'happy') -> 1\n",
            "('happy', 'because') -> 1\n",
            "('because', 'I') -> 1\n",
            "('am', 'learning') -> 1\n",
            "\n",
            "Trigrams: \n",
            "('I', 'am', 'happy') -> 1\n",
            "('am', 'happy', 'because') -> 1\n",
            "('happy', 'because', 'I') -> 1\n",
            "('because', 'I', 'am') -> 1\n",
            "('I', 'am', 'learning') -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Word Prediction N-gram model\n"
      ],
      "metadata": {
        "id": "OC4GGx5SHP4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rd\n",
        "from collections import Counter\n",
        "\n",
        "def calc_ngram_freq(n_gram):\n",
        "  pair_freq = defaultdict(int)\n",
        "  for pair in n_gram:\n",
        "    pair_freq[pair] += 1\n",
        "  return pair_freq\n",
        "\n",
        "def pred_next_word(pre_words, model):\n",
        "  last_word = pre_words[-1]\n",
        "  if last_word not in model:\n",
        "    return None\n",
        "  next_word_prob = model[last_word]\n",
        "  print(\"Following are probability of each word: \")\n",
        "  print(model[last_word])\n",
        "  next_word = max(next_word_prob, key = next_word_prob.get)\n",
        "  return next_word\n",
        "\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "tokens = nltk.word_tokenize(sentence.lower())\n",
        "total_word_count = Counter()\n",
        "model = defaultdict(dict)\n",
        "\n",
        "\n",
        "bigram = list(nltk.bigrams(tokens))\n",
        "pair_freq = calc_ngram_freq(bigram)\n",
        "for (w1,w2) in bigram:\n",
        "  total_word_count[w1] += 1\n",
        "for (w1,w2) in pair_freq.keys():\n",
        "  model[w1][w2] = freq/total_word_count[w1]\n",
        "\n",
        "prev_words = input(\"Enter the previous words separated by space: \")\n",
        "next_word = pred_next_word(prev_words.lower().split(), model)\n",
        "print(\"Next probable word is: \", next_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDh1gZ5JF3xX",
        "outputId": "7ffb4214-8100-4b4c-a037-285af41bc4cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: I was I should I was I must I \n",
            "Enter the previous words separated by space: i\n",
            "Following are probability of each word: \n",
            "{'was': 0.25, 'should': 0.25, 'must': 0.25}\n",
            "Next probable word is:  was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordnet"
      ],
      "metadata": {
        "id": "ZOfD1CzAK3nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# nltk.download('omw')\n",
        "word1 = \"keyboard\"\n",
        "\n",
        "SynArray = wordnet.synsets(word1)\n",
        "print(SynArray)\n",
        "\n",
        "for syn in SynArray:\n",
        "  print(syn.name())\n",
        "  print(syn.definition())\n",
        "  print(syn.examples())\n",
        "  print(\"Hypernyms: \",syn.hypernyms())\n",
        "  print(\"Hyponyms: \", syn.hyponyms())\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLa51Q9cKLJn",
        "outputId": "9b855484-1e59-42c5-d76f-21ec9dfab53e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('keyboard.n.01'), Synset('keyboard.n.02')]\n",
            "keyboard.n.01\n",
            "device consisting of a set of keys on a piano or organ or typewriter or typesetting machine or computer or the like\n",
            "[]\n",
            "Hypernyms:  [Synset('device.n.01')]\n",
            "Hyponyms:  [Synset('computer_keyboard.n.01'), Synset('piano_keyboard.n.01'), Synset('typewriter_keyboard.n.01')]\n",
            "\n",
            "\n",
            "keyboard.n.02\n",
            "holder consisting of an arrangement of hooks on which keys or locks can be hung\n",
            "[]\n",
            "Hypernyms:  [Synset('holder.n.01')]\n",
            "Hyponyms:  []\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing Word Distribution\n"
      ],
      "metadata": {
        "id": "u8kW0ZpMMEoE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzvsFfOYLIZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}